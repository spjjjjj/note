{
  "model": "gpt-4",
  "messages": [
    {
      "role": "system",
      "content": "你是一个问答助手"
    },
    {
      "role": "user",
      "content": "你要写一首关于秋天的诗"
    },
   
  ],
  #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  "max_tokens": 1000,   #生成回答的最大 token 数量
  "temperature": 0.8,   #使用什么采样温度，介于 0 和 1 之间。较高的值（如 0.8）将使输出更加随机，而较低的值（如 0.2）将使输出更加集中和确定。
  "n": 1,   #要生成的回答数量
  "stream": true,   #是否以流式方式返回答案
  "top_p": 1,       #使用 nucleus 采样的概率质量阈值，介于 0 和 1 之间。模型将考虑具有最高累计概率质量的标记，直到达到该阈值。
  #一种替代温度采样的方法，称为核采样，其中模型考虑具有 top_p 概率质量的标记的结果。所以 0.1 意味着只考虑构成前 10% 概率质量的标记。
  "presence_penalty": 0,    #对新主题的引入进行惩罚，范围从 -2.0 到 2.0。正值会增加模型谈论新主题的可能性，而负值会减少这种可能性。
  "frequency_penalty": 0    #对重复使用相同标记进行惩罚，范围从 -2.0 到 2.0。正值会减少模型重复相同行话或短语的可能性，而负值会增加这种可能性。
  #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  }